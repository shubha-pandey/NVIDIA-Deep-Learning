{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks\n",
    "\n",
    "#### Objectives\n",
    "\n",
    "* Prep data specifically for a CNN\n",
    "* Create a more sophisticated CNN model, understanding a greater variety of model layers\n",
    "* Train a CNN model and observe its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and Prepare Data\n",
    "\n",
    "##### Prepare Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(r\"D:\\New folder (4)\\Deep Learning\\data\\AmericanSignLang\\asl_data\\sign_mnist_train.csv\")\n",
    "valid_df = pd.read_csv(r\"D:\\New folder (4)\\Deep Learning\\data\\AmericanSignLang\\asl_data\\sign_mnist_valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ASL data is already flattened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[107, 118, 127, ..., 204, 203, 202],\n",
       "       [155, 157, 156, ..., 103, 135, 149],\n",
       "       [187, 188, 188, ..., 195, 194, 195],\n",
       "       [211, 211, 212, ..., 222, 229, 163],\n",
       "       [164, 167, 170, ..., 163, 164, 179]], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = train_df.head().copy()  # Grab the top 5 rows\n",
    "sample_df.pop('label')\n",
    "sample_x = sample_df.values\n",
    "sample_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this format, there isn't all the information about which pixels are near each other. Because of this, convolutions that will detect features can't be applied.\n",
    "\n",
    "[Reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) the dataset so that they are in a 28x28 pixel format. This will allow the convolutions to associate groups of pixels and detect important features.\n",
    "\n",
    "Note that for the first convolutional layer of the model, not only the height and width of the image, but also the number of [color channels](https://www.photoshopessentials.com/essentials/rgb/) is needed. The images are grayscale, so there's just have 1 channel.\n",
    "\n",
    "That means that it is required to convert the current shape `(5, 784)` to `(5, 1, 28, 28)`. \n",
    "\n",
    "With [NumPy](https://numpy.org/doc/stable/index.html) arrays, pass a `-1` for any dimension to remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_HEIGHT = 28\n",
    "IMG_WIDTH = 28\n",
    "IMG_CHS = 1\n",
    "\n",
    "sample_x = sample_x.reshape(-1, IMG_CHS, IMG_HEIGHT, IMG_WIDTH)\n",
    "sample_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, base_df):\n",
    "        x_df = base_df.copy()  # Some operations below are in-place\n",
    "        y_df = x_df.pop('label')\n",
    "        x_df = x_df.values / 255  # Normalize values from 0 to 1\n",
    "        x_df = x_df.reshape(-1, IMG_CHS, IMG_WIDTH, IMG_HEIGHT)\n",
    "        self.xs = torch.tensor(x_df).float()\n",
    "        self.ys = torch.tensor(y_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.xs[idx]\n",
    "        y = self.ys[idx]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = MyDataset(train_df)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_N = len(train_loader.dataset)\n",
    "\n",
    "valid_data = MyDataset(valid_df)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "valid_N = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.8549, 0.8627, 0.8627,  ..., 0.8667, 0.8627, 0.8588],\n",
       "           [0.8627, 0.8667, 0.8667,  ..., 0.8706, 0.8706, 0.8667],\n",
       "           [0.8706, 0.8745, 0.8824,  ..., 0.8745, 0.8667, 0.8706],\n",
       "           ...,\n",
       "           [0.6118, 0.6235, 0.6392,  ..., 0.9294, 0.9020, 0.8941],\n",
       "           [0.5412, 0.5451, 0.5608,  ..., 0.8627, 0.9255, 0.9059],\n",
       "           [0.5569, 0.5608, 0.5725,  ..., 0.8588, 0.9137, 0.9137]]],\n",
       " \n",
       " \n",
       "         [[[0.8745, 0.8824, 0.8902,  ..., 0.8314, 0.8235, 0.8196],\n",
       "           [0.8824, 0.8941, 0.8980,  ..., 0.8392, 0.8275, 0.8196],\n",
       "           [0.8863, 0.8941, 0.8980,  ..., 0.8392, 0.8353, 0.8235],\n",
       "           ...,\n",
       "           [0.9216, 0.9294, 0.9490,  ..., 0.9137, 0.9020, 0.8980],\n",
       "           [0.9216, 0.9333, 0.9451,  ..., 0.9137, 0.8980, 0.8902],\n",
       "           [0.9216, 0.9294, 0.9412,  ..., 0.9137, 0.9020, 0.8863]]],\n",
       " \n",
       " \n",
       "         [[[0.5725, 0.5882, 0.5922,  ..., 0.5922, 0.5882, 0.5843],\n",
       "           [0.5765, 0.5882, 0.6000,  ..., 0.5961, 0.5922, 0.5882],\n",
       "           [0.5843, 0.5922, 0.6000,  ..., 0.6000, 0.6000, 0.5922],\n",
       "           ...,\n",
       "           [0.6471, 0.7020, 0.5451,  ..., 0.6745, 0.6706, 0.6667],\n",
       "           [0.6863, 0.6039, 0.2549,  ..., 0.6784, 0.6745, 0.6667],\n",
       "           [0.6549, 0.3333, 0.2235,  ..., 0.6824, 0.6745, 0.6667]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.7569, 0.7569, 0.7608,  ..., 0.7647, 0.7647, 0.7608],\n",
       "           [0.7608, 0.7608, 0.7647,  ..., 0.7765, 0.7765, 0.7686],\n",
       "           [0.7647, 0.7647, 0.7647,  ..., 0.7882, 0.7843, 0.7804],\n",
       "           ...,\n",
       "           [0.8510, 0.8549, 0.8627,  ..., 0.8824, 0.8824, 0.8824],\n",
       "           [0.8510, 0.8510, 0.8510,  ..., 0.8824, 0.8824, 0.8824],\n",
       "           [0.8510, 0.8549, 0.8588,  ..., 0.8824, 0.8824, 0.8824]]],\n",
       " \n",
       " \n",
       "         [[[0.5137, 0.5216, 0.5333,  ..., 0.5255, 0.5176, 0.5137],\n",
       "           [0.5255, 0.5373, 0.5412,  ..., 0.5333, 0.5255, 0.5216],\n",
       "           [0.5333, 0.5412, 0.5490,  ..., 0.5373, 0.5333, 0.5294],\n",
       "           ...,\n",
       "           [0.5765, 0.3529, 0.7216,  ..., 0.6941, 0.6784, 0.6706],\n",
       "           [0.3961, 0.0667, 0.6235,  ..., 0.6941, 0.6824, 0.6745],\n",
       "           [0.3373, 0.2235, 0.2824,  ..., 0.6863, 0.6824, 0.6745]]],\n",
       " \n",
       " \n",
       "         [[[0.8471, 0.8510, 0.8510,  ..., 0.3412, 0.3373, 0.6745],\n",
       "           [0.8549, 0.8549, 0.8627,  ..., 0.5059, 0.3216, 0.6471],\n",
       "           [0.8627, 0.8588, 0.8627,  ..., 0.6157, 0.3843, 0.6196],\n",
       "           ...,\n",
       "           [0.5176, 0.8706, 0.8549,  ..., 0.8510, 0.8510, 0.6471],\n",
       "           [0.7176, 0.8627, 0.8510,  ..., 0.8824, 0.8353, 0.8667],\n",
       "           [0.8314, 0.8667, 0.8784,  ..., 0.8863, 0.8235, 0.8235]]]]),\n",
       " tensor([22,  2, 23,  0,  3, 19,  6, 16,  6,  3,  8,  8, 18, 19,  5,  2, 17,  8,\n",
       "          9, 22, 12,  8,  5, 21, 23,  9, 12, 14, 12,  4, 10, 18])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Convolutional Model\n",
    "\n",
    "These days, many data scientists start their projects by borrowing model properties from a similar project. \n",
    "\n",
    "Assuming the problem is not totally unique, there's a great chance that people have created models that will perform well which are posted in online repositories like [TensorFlow Hub](https://www.tensorflow.org/hub) and the [NGC Catalog](https://ngc.nvidia.com/catalog/models). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 24\n",
    "kernel_size = 3\n",
    "flattened_img_size = 75 * 3 * 3\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # First convolution\n",
    "    nn.Conv2d(IMG_CHS, 25, kernel_size, stride=1, padding=1),  # 25 x 28 x 28\n",
    "    nn.BatchNorm2d(25),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, stride=2),  # 25 x 14 x 14\n",
    "    # Second convolution\n",
    "    nn.Conv2d(25, 50, kernel_size, stride=1, padding=1),  # 50 x 14 x 14\n",
    "    nn.BatchNorm2d(50),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(.2),\n",
    "    nn.MaxPool2d(2, stride=2),  # 50 x 7 x 7\n",
    "    # Third convolution\n",
    "    nn.Conv2d(50, 75, kernel_size, stride=1, padding=1),  # 75 x 7 x 7\n",
    "    nn.BatchNorm2d(75),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, stride=2),  # 75 x 3 x 3\n",
    "    # Flatten to Dense\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(flattened_img_size, 512),\n",
    "    nn.Dropout(.3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, n_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conv2D\n",
    "Small kernels will go over the input image and detect features that are important for classification. \n",
    "\n",
    "Earlier convolutions in the model will detect simple features such as lines. \n",
    "\n",
    "Later convolutions will detect more complex features. \n",
    "\n",
    "The first Conv2D layer:\n",
    "```Python\n",
    "nn.Conv2d(IMG_CHS, 25, kernel_size, stride=1, padding=1)\n",
    "```\n",
    "* 25 refers to the number of filters that will be learned. \n",
    "* Even though `kernel_size = 3`, PyTorch will assume we want 3 x 3 filters. \n",
    "* Stride refer to the step size that the filter will take as it passes over the image. \n",
    "* Padding refers to whether the output image that's created from the filter will match the size of the input image.\n",
    "\n",
    "\n",
    "##### BatchNormalization\n",
    "Like normalizing  inputs, batch normalization scales the values in the hidden layers to improve training. [Read more about it in detail here](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/).\n",
    "\n",
    "##### MaxPool2D\n",
    "`ImageMax` pooling takes an image and essentially shrinks it to a lower resolution. It does this to help the model be robust to translation (objects moving side to side), and also makes the model faster.\n",
    "\n",
    "##### Dropout\n",
    "`ImageDropout` is a technique for preventing overfitting. \n",
    "`Dropout` randomly selects a subset of neurons and turns them off, so that they do not participate in forward or backward propagation in that particular pass\n",
    "This helps to make sure that the network is robust and redundant, and does not rely on any one area to come up with answers. \n",
    "\n",
    "##### Flatten\n",
    "`Flatten` takes the output of one layer which is multidimensional, and flattens it into a one-dimensional array. The output is called a feature vector and will be connected to the final classification layer.\n",
    "\n",
    "##### Linear\n",
    "The `first dense layer` (512 units) takes the feature vector as input and learns which features will contribute to a particular classification. \n",
    "\n",
    "The `second dense layer` (24 units) is the final classification layer that outputs our prediction.\n",
    "\n",
    "#### Summarize the Model\n",
    "It's not critical that to understand everything right now in order to effectively train convolutional models. \n",
    "Most importantly keep in mind that they can help with extracting useful information from images, and can be used in classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=Sequential\n",
       "  (0): RecursiveScriptModule(original_name=Conv2d)\n",
       "  (1): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "  (2): RecursiveScriptModule(original_name=ReLU)\n",
       "  (3): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "  (4): RecursiveScriptModule(original_name=Conv2d)\n",
       "  (5): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "  (6): RecursiveScriptModule(original_name=ReLU)\n",
       "  (7): RecursiveScriptModule(original_name=Dropout)\n",
       "  (8): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "  (9): RecursiveScriptModule(original_name=Conv2d)\n",
       "  (10): RecursiveScriptModule(original_name=BatchNorm2d)\n",
       "  (11): RecursiveScriptModule(original_name=ReLU)\n",
       "  (12): RecursiveScriptModule(original_name=MaxPool2d)\n",
       "  (13): RecursiveScriptModule(original_name=Flatten)\n",
       "  (14): RecursiveScriptModule(original_name=Linear)\n",
       "  (15): RecursiveScriptModule(original_name=Dropout)\n",
       "  (16): RecursiveScriptModule(original_name=ReLU)\n",
       "  (17): RecursiveScriptModule(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile the model\n",
    "model = torch.jit.script(model)    # torch.compile is not supported on Windows\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_accuracy(output, y, N):\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "    return correct / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model\n",
    "\n",
    "These are the same `train` and `validate` functions as before, but they have been mixed up. \n",
    "\n",
    "One of them should have `model.train` and the other should have `model.eval`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        output = model(x)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = loss_function(output, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        accuracy += get_batch_accuracy(output, y, train_N)\n",
    "    print('Train - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in train_loader:\n",
    "            output = model(x)\n",
    "\n",
    "            loss += loss_function(output, y).item()\n",
    "            accuracy += get_batch_accuracy(output, y, valid_N)\n",
    "    print('Validation - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "FIXME - Loss: 280.8473 Accuracy: 0.9034\n",
      "FIXME - Loss: 105.9261 Accuracy: 3.7065\n",
      "Epoch: 1\n",
      "FIXME - Loss: 17.8795 Accuracy: 0.9950\n",
      "FIXME - Loss: 6.1149 Accuracy: 3.8222\n",
      "Epoch: 2\n",
      "FIXME - Loss: 9.8100 Accuracy: 0.9969\n",
      "FIXME - Loss: 1.6797 Accuracy: 3.8278\n",
      "Epoch: 3\n",
      "FIXME - Loss: 10.2877 Accuracy: 0.9964\n",
      "FIXME - Loss: 26.6138 Accuracy: 3.7875\n",
      "Epoch: 4\n",
      "FIXME - Loss: 10.5685 Accuracy: 0.9962\n",
      "FIXME - Loss: 0.1757 Accuracy: 3.8281\n",
      "Epoch: 5\n",
      "FIXME - Loss: 8.3664 Accuracy: 0.9972\n",
      "FIXME - Loss: 1.1414 Accuracy: 3.8270\n",
      "Epoch: 6\n",
      "FIXME - Loss: 6.0007 Accuracy: 0.9979\n",
      "FIXME - Loss: 1.5657 Accuracy: 3.8270\n",
      "Epoch: 7\n",
      "FIXME - Loss: 2.9410 Accuracy: 0.9989\n",
      "FIXME - Loss: 4.0265 Accuracy: 3.8242\n",
      "Epoch: 8\n",
      "FIXME - Loss: 9.6840 Accuracy: 0.9963\n",
      "FIXME - Loss: 0.1489 Accuracy: 3.8281\n",
      "Epoch: 9\n",
      "FIXME - Loss: 0.2519 Accuracy: 1.0000\n",
      "FIXME - Loss: 0.0099 Accuracy: 3.8281\n",
      "Epoch: 10\n",
      "FIXME - Loss: 10.8395 Accuracy: 0.9962\n",
      "FIXME - Loss: 0.5731 Accuracy: 3.8278\n",
      "Epoch: 11\n",
      "FIXME - Loss: 0.5971 Accuracy: 0.9998\n",
      "FIXME - Loss: 0.0363 Accuracy: 3.8281\n",
      "Epoch: 12\n",
      "FIXME - Loss: 6.3942 Accuracy: 0.9980\n",
      "FIXME - Loss: 0.1282 Accuracy: 3.8281\n",
      "Epoch: 13\n",
      "FIXME - Loss: 4.9387 Accuracy: 0.9985\n",
      "FIXME - Loss: 0.0387 Accuracy: 3.8281\n",
      "Epoch: 14\n",
      "FIXME - Loss: 4.0150 Accuracy: 0.9986\n",
      "FIXME - Loss: 0.7149 Accuracy: 3.8278\n",
      "Epoch: 15\n",
      "FIXME - Loss: 3.7021 Accuracy: 0.9987\n",
      "FIXME - Loss: 0.0282 Accuracy: 3.8281\n",
      "Epoch: 16\n",
      "FIXME - Loss: 2.9854 Accuracy: 0.9992\n",
      "FIXME - Loss: 15.8616 Accuracy: 3.8056\n",
      "Epoch: 17\n",
      "FIXME - Loss: 2.9203 Accuracy: 0.9990\n",
      "FIXME - Loss: 19.2769 Accuracy: 3.7956\n",
      "Epoch: 18\n",
      "FIXME - Loss: 6.3454 Accuracy: 0.9977\n",
      "FIXME - Loss: 0.0589 Accuracy: 3.8281\n",
      "Epoch: 19\n",
      "FIXME - Loss: 0.4856 Accuracy: 0.9997\n",
      "FIXME - Loss: 0.0027 Accuracy: 3.8281\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    train()\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "This model is significantly improved! The training accuracy is very high, and the validation accuracy has improved as well. This is a great result, as all that had done was swap in a new model.\n",
    "\n",
    "The validation accuracy is jumping around. This is an indication that the model is still not generalizing perfectly.\n",
    "\n",
    "#### Clear the Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
